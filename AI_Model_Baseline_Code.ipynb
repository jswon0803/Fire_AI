{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a06cb40",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a781076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\programming\\miniconda\\envs\\py310\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Path to the directory where the file is saved.\n",
    "directory_path = './001.Training_data'\n",
    "\n",
    "# Retrieve the list of files in the directory.\n",
    "filelist = os.listdir(directory_path)\n",
    "\n",
    "# Filter the file list to include only CSV files.\n",
    "csv_files = [file for file in filelist if file.endswith('.csv')]\n",
    "dataframes = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.iloc[:,:-9]\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a79070",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.concat([dataframes[0], dataframes[1], dataframes[2], dataframes[3],dataframes[4]],axis=0)\n",
    "merge_df['Fire_status'] = merge_df['Fire_status'].map({'Non-accident': 0, 'accident': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3b8dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29108\\1017726561.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(-9999, inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29108\\1017726561.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('-9999', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def replace_missing(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64 or df[col].dtype == np.int64:\n",
    "            df[col].fillna(-9999, inplace=True)\n",
    "        elif df[col].dtype == object:\n",
    "            df[col].fillna('-9999', inplace=True)\n",
    "        elif np.issubdtype(df[col].dtype, np.number) and np.isinf(df[col]).any():\n",
    "            df[col].replace([np.inf, -np.inf], -9999, inplace=True)\n",
    "\n",
    "replace_missing(merge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1744b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the values in column 0 to 1 and 0.\n",
    "merge_df['Fire_status'] = merge_df['Fire_status'].map({1: 'accident', 0: 'Non-accident'})\n",
    "\n",
    "# Convert the remaining object-type columns to dummy variables.\n",
    "object_cols = [col for col in merge_df.columns if merge_df[col].dtype == 'object' and col != 'Fire_status']\n",
    "merge_df = pd.get_dummies(merge_df, columns=object_cols, drop_first=True)\n",
    "\n",
    "# Extract the numerical variables.\n",
    "numerical_cols = merge_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merge_df[numerical_cols] = scaler.fit_transform(merge_df[numerical_cols])\n",
    "\n",
    "# under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(merge_df.drop('Fire_status', axis=1), merge_df['Fire_status'])\n",
    "y_resampled = y_resampled.apply(lambda x: 1 if x == 'accident' else 0)\n",
    "\n",
    "# After under-sampling, split the data into a training set (80%) and a test set (20%).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6209f93",
   "metadata": {},
   "source": [
    "# 0. Predict Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdfafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = dataframes[-1]\n",
    "object_cols = [col for col in df5.columns if df5[col].dtype == 'object' and col != 'Fire_status']\n",
    "df5 = pd.get_dummies(df5, columns=object_cols, drop_first=True)\n",
    "\n",
    "# Replace missing values using a function call.\n",
    "replace_missing(df5)\n",
    "\n",
    "\n",
    "# Extract the numerical variables.\n",
    "numerical_cols = df5.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "df5[numerical_cols] = scaler.fit_transform(df5[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8390ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['B_usg_가설건축물', 'B_usg_생활편익시설', 'L_zon2_유통상업지역', 'L_cond_발전소', 'L_cond_주상복합용', 'L_shape_삼각형', 'L_shape_역삼각형']\n",
    "\n",
    "for column in new_columns:\n",
    "    df5[column] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86669814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['Fire_status'] = df5['Fire_status'].apply(lambda x: 0 if x == 'Non-accident' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7538d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns of `df5` according to the order of the columns in `df1`.\n",
    "df5_reordered = df5[merge_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "876e730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_X_train = df5_reordered.iloc[:, 1:]\n",
    "df5_y_result = df5_reordered.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429e29d",
   "metadata": {},
   "source": [
    " ## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import io  # Import the 'io' module instead of 'sklearn.externals.six'\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Decision Tree Model\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "min_cp = 0.01 #min(DT.cost_complexity_pruning_path(X_train, y_train).ccp_alphas)\n",
    "DT_model = DecisionTreeClassifier(ccp_alpha=min_cp)\n",
    "DT_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Decision Tree Evaluation\n",
    "real_sim_DT = DT_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "DT_cmtx = confusion_matrix(y_test, real_sim_DT)\n",
    "accuracy = accuracy_score(y_test, real_sim_DT)\n",
    "precision = precision_score(y_test, real_sim_DT)\n",
    "recall = recall_score(y_test, real_sim_DT)\n",
    "f1 = f1_score(y_test, real_sim_DT)\n",
    "\n",
    "df5_predict = DT_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_DT.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['DT']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(DT_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame(DT_model.feature_importances_, index=X_train.columns, columns=['Importance']).to_excel(writer, sheet_name='Feature Importance')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_DT}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "    \n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['DT'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_DT.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'DT_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_DT.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8da733",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Random Forest Model\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Evaluation\n",
    "real_sim_RF = RF.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "RF_cmtx = confusion_matrix(y_test, real_sim_RF)\n",
    "accuracy = accuracy_score(y_test, real_sim_RF)\n",
    "precision = precision_score(y_test, real_sim_RF)\n",
    "recall = recall_score(y_test, real_sim_RF)\n",
    "f1 = f1_score(y_test, real_sim_RF)\n",
    "\n",
    "df5_predict = RF.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_RF.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['RF']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(RF_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame(RF.feature_importances_, index=X_train.columns, columns=['Importance']).to_excel(writer, sheet_name='Feature Importance',)\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_RF}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['RF'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_RF.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'RF_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_RF.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed7396",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB  # Changed from GaussianNB to BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Bayesian Model (Bernoulli Naive Bayes)\n",
    "BNB = BernoulliNB()  # Changed to BernoulliNB\n",
    "BNB.fit(X_train, y_train)\n",
    "\n",
    "# Bayesian Model Evaluation\n",
    "real_sim_BNB = BNB.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "BNB_cmtx = confusion_matrix(y_test, real_sim_BNB)\n",
    "accuracy = accuracy_score(y_test, real_sim_BNB)\n",
    "precision = precision_score(y_test, real_sim_BNB)\n",
    "recall = recall_score(y_test, real_sim_BNB)\n",
    "f1 = f1_score(y_test, real_sim_BNB)\n",
    "\n",
    "\n",
    "\n",
    "df5_predict = BNB.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_BNB.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['NB']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(BNB_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    # Feature importance is not applicable for Bernoulli Naive Bayes\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_BNB}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['NB'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_NB.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'NB_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_NB.csv\", mode='a', header=True, index=False)\n",
    "    \n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc344a4",
   "metadata": {},
   "source": [
    "## 4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "#KNN은 numpy형태로 x_train을 만들어야합니다. 따라서 해당코드실행뒤 다른 모델을 train하기위해선 다시  데이터프레임형태로 만들어야합니다.\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "# 위코드를 실행뒤 다른 코드를 진행하셔야합니다!\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.ascontiguousarray(X_resampled), y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# KNN Model\n",
    "k_value = 5  # You can choose the appropriate k value\n",
    "KNN = KNeighborsClassifier(n_neighbors=k_value)\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "# KNN Model Evaluation\n",
    "real_sim_KNN = KNN.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "KNN_cmtx = confusion_matrix(y_test, real_sim_KNN)\n",
    "accuracy = accuracy_score(y_test, real_sim_KNN)\n",
    "precision = precision_score(y_test, real_sim_KNN)\n",
    "recall = recall_score(y_test, real_sim_KNN)\n",
    "f1 = f1_score(y_test, real_sim_KNN)\n",
    "\n",
    "\n",
    "df5_predict = KNN.predict((np.ascontiguousarray(df5_X_train)))\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_KNN.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['KNN']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(KNN_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_KNN}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "    \n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['KNN'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_KNN.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'KNN_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_KNN.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63026260",
   "metadata": {},
   "source": [
    "## 5. SVM (kernel = radial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d165845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume X_resampled and y_resampled are your feature and target columns\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# RBF SVM Model\n",
    "rbf_SVM = SVC(kernel='rbf')\n",
    "rbf_SVM.fit(X_train, y_train)\n",
    "\n",
    "# RBF SVM Model Evaluation\n",
    "real_sim_rbf_SVM = rbf_SVM.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "rbf_SVM_cmtx = confusion_matrix(y_test, real_sim_rbf_SVM)\n",
    "accuracy = accuracy_score(y_test, real_sim_rbf_SVM)\n",
    "precision = precision_score(y_test, real_sim_rbf_SVM)\n",
    "recall = recall_score(y_test, real_sim_rbf_SVM)\n",
    "f1 = f1_score(y_test, real_sim_rbf_SVM)\n",
    "\n",
    "print('1차')\n",
    "\n",
    "df5_predict = rbf_SVM.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_rbf_SVM.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['RBF SVM']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(rbf_SVM_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_rbf_SVM}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['RBF SVM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_RBF_SVM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'RBF_SVM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_RBF_SVM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "    \n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b92a47",
   "metadata": {},
   "source": [
    "## 6. SVM (kernel = poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Polynomial SVM Model\n",
    "poly_SVM = SVC(kernel='poly', degree=3)  # You can adjust the degree parameter\n",
    "poly_SVM.fit(X_train, y_train)\n",
    "\n",
    "# Polynomial SVM Model Evaluation\n",
    "real_sim_poly_SVM = poly_SVM.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "poly_SVM_cmtx = confusion_matrix(y_test, real_sim_poly_SVM)\n",
    "accuracy = accuracy_score(y_test, real_sim_poly_SVM)\n",
    "precision = precision_score(y_test, real_sim_poly_SVM)\n",
    "recall = recall_score(y_test, real_sim_poly_SVM)\n",
    "f1 = f1_score(y_test, real_sim_poly_SVM)\n",
    "\n",
    "print('1차')\n",
    "\n",
    "df5_predict = poly_SVM.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_poly_SVM.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['Polynomial SVM']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(poly_SVM_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_poly_SVM}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['Polynomial SVM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_Polynomial_SVM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'Polynomial_SVM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_Polynomial_SVM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f71a2a",
   "metadata": {},
   "source": [
    "## 7. SVM (kernel = sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Sigmoid SVM Model\n",
    "sigmoid_SVM = SVC(kernel='sigmoid')\n",
    "sigmoid_SVM.fit(X_train, y_train)\n",
    "\n",
    "# Sigmoid SVM Model Evaluation\n",
    "real_sim_sigmoid_SVM = sigmoid_SVM.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "sigmoid_SVM_cmtx = confusion_matrix(y_test, real_sim_sigmoid_SVM)\n",
    "accuracy = accuracy_score(y_test, real_sim_sigmoid_SVM)\n",
    "precision = precision_score(y_test, real_sim_sigmoid_SVM)\n",
    "recall = recall_score(y_test, real_sim_sigmoid_SVM)\n",
    "f1 = f1_score(y_test, real_sim_sigmoid_SVM)\n",
    "\n",
    "print('1차')\n",
    "\n",
    "df5_predict = sigmoid_SVM.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_sigmoid_SVM.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['Sigmoid SVM']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(sigmoid_SVM_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_sigmoid_SVM}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['Sigmoid SVM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_Sigmoid_SVM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'Sigmoid_SVM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_Sigmoid_SVM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85162c",
   "metadata": {},
   "source": [
    "## 8. SVM (kernel = linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Linear SVM Model\n",
    "linear_SVM = SVC(kernel='linear')\n",
    "linear_SVM.fit(X_train, y_train)\n",
    "\n",
    "# Linear SVM Model Evaluation\n",
    "real_sim_linear_SVM = linear_SVM.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "linear_SVM_cmtx = confusion_matrix(y_test, real_sim_linear_SVM)\n",
    "accuracy = accuracy_score(y_test, real_sim_linear_SVM)\n",
    "precision = precision_score(y_test, real_sim_linear_SVM)\n",
    "recall = recall_score(y_test, real_sim_linear_SVM)\n",
    "f1 = f1_score(y_test, real_sim_linear_SVM)\n",
    "\n",
    "\n",
    "df5_predict = linear_SVM.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_linear_SVM.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['Linear SVM']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(linear_SVM_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_linear_SVM}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['Linear SVM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_Linear_SVM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'Linear_SVM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_Linear_SVM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868aacbf",
   "metadata": {},
   "source": [
    "## 9. BA(BaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Bagging Classifier (Random Forest)\n",
    "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Bagging Model Evaluation\n",
    "real_sim_bagging = bagging_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "bagging_cmtx = confusion_matrix(y_test, real_sim_bagging)\n",
    "accuracy = accuracy_score(y_test, real_sim_bagging)\n",
    "precision = precision_score(y_test, real_sim_bagging)\n",
    "recall = recall_score(y_test, real_sim_bagging)\n",
    "f1 = f1_score(y_test, real_sim_bagging)\n",
    "\n",
    "df5_predict = bagging_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_bagging.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['BA']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(bagging_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_bagging}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['BA'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_BA.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'BA_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_BA.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d078b",
   "metadata": {},
   "source": [
    "## 10. Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# AdaBoost Classifier\n",
    "base_estimator = DecisionTreeClassifier()  # You can customize the base estimator\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost Model Evaluation\n",
    "real_sim_adaboost = adaboost_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, real_sim_adaboost)\n",
    "precision = precision_score(y_test, real_sim_adaboost)\n",
    "recall = recall_score(y_test, real_sim_adaboost)\n",
    "f1 = f1_score(y_test, real_sim_adaboost)\n",
    "\n",
    "# Access feature importances of the first base estimator after fitting\n",
    "first_base_estimator = adaboost_model.estimators_[0]  \n",
    "feature_importance = first_base_estimator.feature_importances_\n",
    "feature_names = df5_X_train.columns\n",
    "\n",
    "df5_predict = adaboost_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_adaboost.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['AdaBoost']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance}).to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "    pd.DataFrame(confusion_matrix(y_test, real_sim_adaboost), index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_adaboost}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['AdaBoost'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_ADA.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'ADA_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_ADA.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559bcdf",
   "metadata": {},
   "source": [
    "## 11. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba7cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# CatBoost Classifier\n",
    "catboost_model = CatBoostClassifier(iterations=100, random_state=42)\n",
    "catboost_model.fit(X_train, y_train, cat_features=[])\n",
    "\n",
    "# CatBoost Model Evaluation\n",
    "real_sim_catboost = catboost_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "catboost_cmtx = confusion_matrix(y_test, real_sim_catboost)\n",
    "accuracy = accuracy_score(y_test, real_sim_catboost)\n",
    "precision = precision_score(y_test, real_sim_catboost)\n",
    "recall = recall_score(y_test, real_sim_catboost)\n",
    "f1 = f1_score(y_test, real_sim_catboost)\n",
    "\n",
    "\n",
    "df5_predict = catboost_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_catboost.xlsx\") as writer:\n",
    "    pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['CatBoost']).to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "    pd.DataFrame(catboost_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "  \n",
    "    # Save Feature Importance\n",
    "    feature_importance_df = pd.DataFrame({'Feature': df5_X_train.columns, 'Importance': catboost_model.feature_importances_})\n",
    "    feature_importance_df.to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "    pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_catboost}).to_excel(writer, sheet_name='Test Data Predict')\n",
    "    \n",
    "    \n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['CatBoost'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_CAT.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'CAT_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_CAT.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94dadba",
   "metadata": {},
   "source": [
    "## 12. Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db135174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost Model Evaluation\n",
    "real_sim_xgboost = xgboost_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "xgboost_cmtx = confusion_matrix(y_test, real_sim_xgboost)\n",
    "accuracy = accuracy_score(y_test, real_sim_xgboost)\n",
    "precision = precision_score(y_test, real_sim_xgboost)\n",
    "recall = recall_score(y_test, real_sim_xgboost)\n",
    "f1 = f1_score(y_test, real_sim_xgboost)\n",
    "\n",
    "\n",
    "\n",
    "df5_predict = xgboost_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_xgboost.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['XGBoost'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(xgboost_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # Save Feature Importance\n",
    "    feature_importance_df = pd.DataFrame({'Feature': df5_X_train.columns, 'Importance': xgboost_model.feature_importances_})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    feature_importance_df.to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "\n",
    "    # Save Test Data Predictions\n",
    "    test_data_df = pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_xgboost})\n",
    "    test_data_df.to_excel(writer, sheet_name='Test Data Predict', index=False)\n",
    "\n",
    "        \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['XGBoost'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_XG.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'XG_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_XG.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacc322",
   "metadata": {},
   "source": [
    "## 13. GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "# GBM Classifier\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# GBM Model Evaluation\n",
    "real_sim_gbm = gbm_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "gbm_cmtx = confusion_matrix(y_test, real_sim_gbm)\n",
    "accuracy = accuracy_score(y_test, real_sim_gbm)\n",
    "precision = precision_score(y_test, real_sim_gbm)\n",
    "recall = recall_score(y_test, real_sim_gbm)\n",
    "f1 = f1_score(y_test, real_sim_gbm)\n",
    "\n",
    "\n",
    "df5_predict = gbm_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_gbm.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['GBM'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(gbm_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # Save Feature Importance\n",
    "    feature_importance_df = pd.DataFrame({'Feature': df5_X_train.columns, 'Importance': gbm_model.feature_importances_})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    feature_importance_df.to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "\n",
    "    # Save Test Data Predictions\n",
    "    test_data_df = pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_gbm})\n",
    "    test_data_df.to_excel(writer, sheet_name='Test Data Predict', index=False)\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['GBM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_GBM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'GBM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_GBM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "\n",
    "    # Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d95ab",
   "metadata": {},
   "source": [
    "## 14. ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a86650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "df5_X_train_scaled = scaler.transform(df5_X_train)  # 추가: df5_X_train도 스케일링\n",
    "\n",
    "# DNN Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# DNN Model Evaluation\n",
    "real_sim_dnn = (model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Confusion Matrix\n",
    "dnn_cmtx = confusion_matrix(y_test, real_sim_dnn)\n",
    "accuracy = accuracy_score(y_test, real_sim_dnn)\n",
    "precision = precision_score(y_test, real_sim_dnn)\n",
    "recall = recall_score(y_test, real_sim_dnn)\n",
    "f1 = f1_score(y_test, real_sim_dnn)\n",
    "\n",
    "# df5 데이터 예측 및 평가\n",
    "df5_predict_prob = model.predict(df5_X_train_scaled)\n",
    "df5_predict = (df5_predict_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_ann.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['ANN'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(dnn_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # ... (other sheets if needed)\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['ANN'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_ANN.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'ANN_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_ANN.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "    \n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f86647",
   "metadata": {},
   "source": [
    "## 14-1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "df5_X_train_scaled = scaler.transform(df5_X_train)  # 추가: df5_X_train도 스케일링\n",
    "\n",
    "# DNN Model (Deeper Neural Network)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(128, activation='relu'))  # Additional hidden layer\n",
    "model.add(Dense(64, activation='relu'))   # Additional hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# DNN Model Evaluation\n",
    "real_sim_dnn = (model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Confusion Matrix\n",
    "dnn_cmtx = confusion_matrix(y_test, real_sim_dnn)\n",
    "accuracy = accuracy_score(y_test, real_sim_dnn)\n",
    "precision = precision_score(y_test, real_sim_dnn)\n",
    "recall = recall_score(y_test, real_sim_dnn)\n",
    "f1 = f1_score(y_test, real_sim_dnn)\n",
    "\n",
    "# df5 데이터 예측 및 평가\n",
    "df5_predict_prob = model.predict(df5_X_train_scaled)\n",
    "df5_predict = (df5_predict_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_dnn.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['DNN'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(dnn_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # ... (other sheets if needed)\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['DNN'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_DNN.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'DNN_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_DNN.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "    \n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d12074",
   "metadata": {},
   "source": [
    "## 15. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "df5_X_train_scaled = scaler.transform(df5_X_train)  # 추가: df5_X_train도 스케일링\n",
    "\n",
    "# Reshape data for LSTM (assuming X_train and X_test are 2D arrays)\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "df5_X_train_reshaped = df5_X_train_scaled.reshape((df5_X_train_scaled.shape[0], 1, df5_X_train_scaled.shape[1]))\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=7, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# LSTM Model Evaluation\n",
    "real_sim_lstm = (model.predict(X_test_reshaped) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Confusion Matrix\n",
    "lstm_cmtx = confusion_matrix(y_test, real_sim_lstm)\n",
    "accuracy = accuracy_score(y_test, real_sim_lstm)\n",
    "precision = precision_score(y_test, real_sim_lstm)\n",
    "recall = recall_score(y_test, real_sim_lstm)\n",
    "f1 = f1_score(y_test, real_sim_lstm)\n",
    "\n",
    "# df5 데이터 예측 및 평가\n",
    "df5_predict_prob = model.predict(df5_X_train_reshaped)\n",
    "df5_predict = (df5_predict_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"./result_summary_lstm.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['LSTM'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(lstm_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    \n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['LSTM'])\n",
    "df5_results.to_csv(\"./11.result/score/final_result_LSTM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'LSTM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_LSTM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4904248",
   "metadata": {},
   "source": [
    "## 16. LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a654b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Record the start time of code execution.\n",
    "start_time = time.time()\n",
    "\n",
    "# LightGBM Classifier\n",
    "lgbm_model = LGBMClassifier()\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# LightGBM Model Evaluation\n",
    "real_sim_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "lgbm_cmtx = confusion_matrix(y_test, real_sim_lgbm)\n",
    "accuracy = accuracy_score(y_test, real_sim_lgbm)\n",
    "precision = precision_score(y_test, real_sim_lgbm)\n",
    "recall = recall_score(y_test, real_sim_lgbm)\n",
    "f1 = f1_score(y_test, real_sim_lgbm)\n",
    "\n",
    "df5_predict = lgbm_model.predict(df5_X_train)\n",
    "df5_accuracy = accuracy_score(df5_y_result, df5_predict)\n",
    "df5_precision = precision_score(df5_y_result, df5_predict)\n",
    "df5_recall = recall_score(df5_y_result, df5_predict)\n",
    "df5_f1 = f1_score(df5_y_result, df5_predict)\n",
    "\n",
    "# Save Results to Excel\n",
    "with pd.ExcelWriter(\"result_summary_lgbm.xlsx\") as writer:\n",
    "    # Save Summary\n",
    "    summary_df = pd.DataFrame({'Accuracy': [accuracy], 'Precision': [precision], 'Recall': [recall], 'F1 Score': [f1]}, index=['LightGBM'])\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index_label='Model')\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    pd.DataFrame(lgbm_cmtx, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive']).to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # Save Feature Importance\n",
    "    feature_importance_df = pd.DataFrame({'Feature': df5_X_train.columns, 'Importance': lgbm_model.feature_importances_})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    feature_importance_df.to_excel(writer, sheet_name='Feature Importance', index=False)\n",
    "\n",
    "    # Save Test Data Predictions\n",
    "    test_data_df = pd.DataFrame({'Actual': y_test, 'Predicted': real_sim_lgbm})\n",
    "    test_data_df.to_excel(writer, sheet_name='Test Data Predict', index=False)\n",
    "\n",
    "df5_results = pd.DataFrame({'Accuracy': [df5_accuracy], 'Precision': [df5_precision], 'Recall': [df5_recall], 'F1 Score': [df5_f1]}, index=['LightGBM'])\n",
    "df5_results.to_csv(\"./11.result/score/score_result_LGBM.csv\", index_label='Model')\n",
    "\n",
    "df5_predictions = pd.DataFrame({'Actual': df5_y_result, 'LGBM_Predicted': df5_predict})\n",
    "df5_predictions.to_csv(\"./11.result/predict/final_result_LGBM.csv\", mode='a', header=True, index=False)\n",
    "\n",
    "# Record the end time of code execution.\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the execution time.\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time.\n",
    "print(f\"Record the start time : {execution_time}초\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[py310]",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
